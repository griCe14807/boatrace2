{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ボートレースのレース情報をクロールしpickleファイルに保存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from datetime import timedelta\n",
    "from http.client import RemoteDisconnected\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib.request\n",
    "import time\n",
    "import pandas as pd\n",
    "import re\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. モジュールをロード\n",
    "### 1.1 web pageから情報を取ってきてpandas dfに格納するモジュール"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1.3 レース結果のページからスクレイプするモジュール\n",
    "- 例：https://boatrace.jp/owpc/pc/race/raceresult?rno=11&jcd=15&hd=20210224"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "def scrape_beforeinfo(soup, rno, jcd, hd):   \n",
    "    # 展示競争のコース・スタートタイムをクロール\n",
    "    race_result_dict_list_2 = []\n",
    "    exhibition_start_rows = soup.find(class_=\"is-w238\").find(class_=\"is-p10-0\").find_all(\"tr\")\n",
    "    \n",
    "    for i, exhibition_start_row in enumerate(exhibition_start_rows, 1):\n",
    "        race_result_dict_2 = {\"date\": \"-\".join([hd[0:4], hd[5:7], hd[8:10]]),\n",
    "                        \"venue\": jcd,\n",
    "                        \"raceNumber\": rno[:-1]\n",
    "                        }\n",
    "\n",
    "        race_result_dict_2[\"exhibition_cource\"] = i\n",
    "        race_result_dict_2[\"枠\"] = int(exhibition_start_row.find(class_=re.compile(\"table1_boatImage1Number\")).text)\n",
    "        race_result_dict_2[\"exhibition_start_time\"] = exhibition_start_row.find(class_=\"table1_boatImage1Time\").text\n",
    "        \n",
    "        race_result_dict_list_2.append(race_result_dict_2)\n",
    "    \n",
    "    # dictを入れたlistをdfに変換\n",
    "    beforeinfo_df = pd.DataFrame.from_dict(race_result_dict_list_2)\n",
    "    \n",
    "    time.sleep(0.1)\n",
    "\n",
    "    return beforeinfo_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 そのほかcrawl, scrapeに必要なモジュール"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_url(crawl_key, rno, jcd, hd):\n",
    "    \"\"\"\n",
    "    :param crawl_key: 何をcrawleするか。選択肢は、\"odds3t\"（オッズ）, \"racelist\"(出走表）,\n",
    "    \"beforeinfo\" (直前情報）もしくは\"raceresult\" (レース結果)\n",
    "    :param rno: レース番号。8Rなど、1-12の数字 + R をstrで\n",
    "    :param jcd: 会場名。\"桐　生\"、\"びわこ\"など\n",
    "    :param hd: holding day (レース開催日)、2019/03/28などyyyy/mm/ddの形で入力（strで）\n",
    "    :return dds_url: 公式サイト最終オッズが書かれているページのurl. これを使ってcrawlする\n",
    "    \"\"\"\n",
    "    jcd_dict =  {\"桐　生\": \"01\", \"戸　田\": \"02\", \"江戸川\": \"03\", \"平和島\": \"04\", \"多摩川\": \"05\", \"浜名湖\": \"06\", \"蒲　郡\": \"07\", \"常　滑\": \"08\",\n",
    "                \"　津　\": \"09\", \"三　国\": \"10\", \"びわこ\": \"11\", \"住之江\": \"12\", \"尼　崎\": \"13\", \"鳴　門\": \"14\", \"丸　亀\": \"15\", \"児　島\": \"16\",\n",
    "                \"宮　島\": \"17\", \"徳　山\": \"18\", \"下　関\": \"19\", \"若　松\": \"20\", \"芦　屋\": \"21\", \"福　岡\": \"22\", \"唐　津\": \"23\", \"大　村\": \"24\"\n",
    "                }\n",
    "    rno = rno[:-1]\n",
    "    hd = hd[0:4] + hd[5:7] + hd[8:10]\n",
    "\n",
    "    odds_url = \"http://boatrace.jp/owpc/pc/race/\" + crawl_key + \"?rno=\" + rno + \"&jcd=\" + jcd_dict[jcd] + \"&hd=\" + hd\n",
    "\n",
    "    return odds_url\n",
    "\n",
    "\n",
    "def html_parser(site_url):\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:47.0) Gecko/20100101 Firefox/47.0\",\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        request = urllib.request.Request(url=site_url, headers=headers)\n",
    "        response = urllib.request.urlopen(request)\n",
    "\n",
    "        html = response.read().decode('utf-8')\n",
    "        soup = BeautifulSoup(html, 'lxml')\n",
    "\n",
    "    # データベース作成の際、remotedisconnectedになった場合,そのレースをパス\n",
    "    except RemoteDisconnected:\n",
    "        print(\"remote disconnected error !\")\n",
    "        return None\n",
    "\n",
    "    except ConnectionResetError:\n",
    "        print(\"Connection Reset error !\")\n",
    "        return None\n",
    "\n",
    "    return soup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 実行\n",
    "- 最初の以下の行にてクロールを行う日付を指定\n",
    "\n",
    "　　　　　　　　`　hd_list = [\"2021/02/0\" + str(day) for day in range(1,10)]`\n",
    "- クロール元：ボートレース 公式サイト（https://boatrace.jp/owpc/pc/race/racelist?rno=12&jcd=01&hd=20210325など）\n",
    "- 保存先：'./crawledData/　以下。日にちごとにファイルを作成し保存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99日分のデータが未収集\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import os\n",
    "race_file_set = set([os.path.basename(file) for file in glob.glob(os.path.join('../../../data/crawledData', '*.pkl'))])\n",
    "start_time_file_set = set([os.path.basename(file) for file in glob.glob(os.path.join('../../../data/crawledData/exhibition_mod', '*.pkl'))])\n",
    "\n",
    "diff_file_name_list = list(race_file_set - start_time_file_set)\n",
    "hd_list = [filename[0:4] + \"/\" + filename[4:6] + \"/\" + filename[6:8] for filename in diff_file_name_list]\n",
    "print(\"{0}日分のデータが未収集\".format(len(hd_list)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020/12/30 のデータをクロール中\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ad749df90914be1b8c39fdc387532a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/24 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021/01/24 のデータをクロール中\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "555337da88c6405ca18841dea828dc16",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/24 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021/01/12 のデータをクロール中\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7056e43e08a347e5921029b3d5782928",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/24 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020/12/23 のデータをクロール中\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "665037b2e3e64bfb840b31b72dd49872",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/24 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "jcd_list =  [\"桐　生\", \"戸　田\", \"江戸川\", \"平和島\", \"多摩川\", \"浜名湖\", \"蒲　郡\", \"常　滑\",\n",
    "                \"　津　\", \"三　国\", \"びわこ\", \"住之江\", \"尼　崎\", \"鳴　門\", \"丸　亀\", \"児　島\",\n",
    "                \"宮　島\", \"徳　山\", \"下　関\", \"若　松\", \"芦　屋\", \"福　岡\", \"唐　津\", \"大　村\"\n",
    "            ]\n",
    "\n",
    "for hd in hd_list:\n",
    "    print(\"{0} のデータをクロール中\".format(hd))\n",
    "\n",
    "    # 1日単位でデータを集めてファイルに保存する\n",
    "    today_race_df_list = []\n",
    "\n",
    "    for jcd in tqdm(jcd_list):\n",
    "        for i in range(1, 13):\n",
    "            rno = str(i) + \"R\"\n",
    "\n",
    "            # その日レースがない場所は飛ばすためのtry-except         \n",
    "            try:\n",
    "                # 色々なkeyに対してクロールして特定のレースの情報がまとまったdfを作る\n",
    "                raceResult_url = make_url(\"beforeinfo\", rno, jcd, hd)\n",
    "\n",
    "                # パース\n",
    "                soup = html_parser(raceResult_url)\n",
    "\n",
    "                # 対象サイトをcrawl\n",
    "                race_information_df = scrape_beforeinfo(soup, rno, jcd, hd)\n",
    "                race_information_df = race_information_df.set_index([\"date\", \"venue\", \"raceNumber\", \"枠\"])\n",
    "\n",
    "                # 今回のレースのデータを本日のデータを集めたリストに格納\n",
    "                today_race_df_list.append(race_information_df)\n",
    "\n",
    "            except IndexError:\n",
    "                # print(hd + \" \" + jcd + rno +\"データなし\")\n",
    "                pass\n",
    "            except AttributeError:\n",
    "                pass\n",
    "\n",
    "    # 本日のレースデータを集めたリストをdfに変換\n",
    "    today_race_df = pd.concat(today_race_df_list, axis = 0)\n",
    "\n",
    "    # pickleファイルで保存\n",
    "    today_race_df.to_pickle('../../../data/crawledData/exhibition_mod/{0}.pkl'.format(\"\".join(hd.split(\"/\"))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ファイル内容確認用\n",
    "df = pd.read_pickle('../../../data/crawledData/exhibition_mod/20201124.pkl')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# crawlerの動きを確認する用\n",
    "\n",
    "crawl_key = \"raceresult\"\n",
    "jcd =  \"大　村\"\n",
    "hd = \"2021/03/31\"\n",
    "rno = \"9R\"\n",
    "\n",
    "raceResult_url = make_url(crawl_key, rno, jcd, hd)\n",
    "print(raceResult_url)\n",
    "\n",
    "# パース\n",
    "soup = html_parser(raceResult_url)\n",
    "\n",
    "# 対象サイトをcrawl\n",
    "race_information_df = scrape_raceresult(soup, rno, jcd, hd)\n",
    "race_information_df = race_information_df.set_index([\"date\", \"venue\", \"raceNumber\", \"枠\"])\n",
    "race_information_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "race_information_df[1]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
