{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ボートレースのレース情報をクロールしpickleファイルに保存\n",
    "- クロール元：ボートレース 公式サイト（https://boatrace.jp/owpc/pc/race/racelist?rno=12&jcd=01&hd=20210325など）\n",
    "- 保存先：'./crawledData/　以下。日にちごとにファイルを作成し保存\n",
    "\n",
    "### 使い方\n",
    "- 上から順番に各種モジュールをロード\n",
    "- 「実行部分」セルの最初の以下の行にてクロールを行う日付を指定\n",
    "\n",
    "　　　　　　　　`　hd_list = [\"2021/02/0\" + str(day) for day in range(1,10)]`\n",
    "### 注意点\n",
    "- （多分）beautifulsoupが新しいversionだとtagを取ってくるときのスペースが無視されてエラーになる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from datetime import timedelta\n",
    "from http.client import RemoteDisconnected\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib.request\n",
    "import time\n",
    "import pandas as pd\n",
    "import re\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### web pageから情報を取ってきてpandas dfに格納するモジュール"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_racelist(soup, rno, jcd, hd):\n",
    "    \"\"\"\n",
    "    racelistのページに書かれている情報をクロール\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    table = soup.find(class_=\"contentsFrame1_inner\").find_all(class_=\"table1\")[1]\n",
    "    rows = table.find_all(\"tbody\", {\"class\": \"is-fs12\"})\n",
    "    \n",
    "    race_result_dict_list = []\n",
    "    \n",
    "    for i, row in enumerate(rows, 1):\n",
    "        race_result_dict = {\"date\": \"-\".join([hd[0:4], hd[5:7], hd[8:10]]),\n",
    "                            \"venue\": jcd, \"raceNumber\": rno[:-1]\n",
    "                           }\n",
    "        # 枠\n",
    "        race_result_dict[\"枠\"] = i\n",
    "        # racer id\n",
    "        race_result_dict[\"racer_id\"] = row.find(class_=\"is-fs11\").text.split(\"\\n\")[1][-6:-2]\n",
    "        race_result_dict[\"racer_class\"] = row.find(class_=\"is-fs11\").text.split(\"\\n\")[2][-2:]\n",
    "\n",
    "        # 選手名。最後の[1:-1]は改行を削除するため\n",
    "        racer_name = row.find(class_=\"is-fs18 is-fBold\").text[1:-1]\n",
    "        \n",
    "        # race_result_listの要素としてクロールした結果のリストを追加\n",
    "        race_result_dict[\"racer_name\"] = racer_name\n",
    "\n",
    "        # racer data\n",
    "        racer_column_3 = row.find_all(\"td\", {\"class\": \"is-lineH2\"})[0].text.split(\"\\n\")\n",
    "        race_result_dict[\"num_false_start\"] = racer_column_3[1][-3:-1]\n",
    "        race_result_dict[\"num_late_start\"] = racer_column_3[2][-3:-1]\n",
    "\n",
    "        # crawl motor data\n",
    "        motor_column = row.find_all(\"td\", {\"class\": \"is-lineH2\"})[3].text.split(\"\\n\")\n",
    "        race_result_dict[\"motorNo\"] = motor_column[1][-4:-1]\n",
    "        race_result_dict[\"モーター2連率\"] = motor_column[2][-7:-1]\n",
    "        race_result_dict[\"モーター3連率\"] = motor_column[3][-7:-1]\n",
    "\n",
    "        # crawl boat data\n",
    "        boat_column = row.find_all(\"td\", {\"class\": \"is-lineH2\"})[4].text.split(\"\\n\")\n",
    "        race_result_dict[\"boatNo\"] = boat_column[1][-4:-1]\n",
    "        race_result_dict[\"ボート2連率\"] = boat_column[2][-7:-1]\n",
    "        race_result_dict[\"ボート3連率\"] = boat_column[3][-7:-1]\n",
    "        \n",
    "        race_result_dict_list.append(race_result_dict)\n",
    "        \n",
    "    # dictをdfに変換\n",
    "    race_result_df = pd.DataFrame.from_dict(race_result_dict_list)\n",
    "\n",
    "    time.sleep(0.1)\n",
    "\n",
    "    return race_result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_beforeinfo(soup, rno, jcd, hd):\n",
    "    \"\"\"\n",
    "    exhibitionの情報など、直前情報ページに書かれている情報をクロール\n",
    "    :param soup:\n",
    "    :param rno:\n",
    "    :param jcd:\n",
    "    :param hd:\n",
    "    :return:\n",
    "\n",
    "    # TODO: プロペラ\n",
    "    # TODO: 部品交換\n",
    "    # TODO: 前走成績\n",
    "    # TODO: 調整重量 (adjustment weight) (kg)\n",
    "    # TODO: 風向き\n",
    "\n",
    "    \"\"\"\n",
    "    race_result_dict_list = []\n",
    "    \n",
    "    table = soup.find(class_=\"contentsFrame1_inner\").find_all(class_=\"table1\")[1]\n",
    "    rows = table.find_all(\"tbody\", {\"class\": \"is-fs12\"})\n",
    "    \n",
    "    table2 = soup.find(class_=\"contentsFrame1_inner\").find_all(class_=\"table1\")[2]\n",
    "    rows2 = table2.find_all(\"tr\")\n",
    "\n",
    "\n",
    "    for i, (row, row2) in enumerate(zip(rows, rows2[2:]), 1):\n",
    "        \n",
    "        race_result_dict = {\"date\": \"-\".join([hd[0:4], hd[5:7], hd[8:10]]),\n",
    "                        \"venue\": jcd,\n",
    "                        \"raceNumber\": rno[:-1]\n",
    "                        }\n",
    "        # 枠\n",
    "        race_result_dict[\"枠\"] = i\n",
    "        \n",
    "        # 水面気象情報\n",
    "        table3 = soup.find(class_=\"contentsFrame1_inner\").find(class_=\"weather1\")\n",
    "        weather_data = (table3.find_all(class_=\"weather1_bodyUnitLabelData\"))\n",
    "        weather_string = table3.find_all(class_=\"weather1_bodyUnitLabelTitle\")\n",
    "\n",
    "        race_result_dict[\"temperature\"] = weather_data[0].text[:-1]\n",
    "        race_result_dict[\"weather\"] = weather_string[1].text\n",
    "        race_result_dict[\"wind_speed\"] = weather_data[1].text[:-1]\n",
    "        race_result_dict[\"water_temperature\"] = weather_data[2].text[:-1]\n",
    "        race_result_dict[\"wave_height\"] = weather_data[3].text[:-2]\n",
    "\n",
    "        # racer weight (kg)\n",
    "        # 書いていないことがあり、その場合エラーになる\n",
    "        race_result_dict[\"weight\"] = row.find(\"td\", {\"rowspan\": \"2\"}).text[:-2]\n",
    "\n",
    "        # 展示タイム\n",
    "        race_result_dict[\"exhibitionTime\"] = row.find_all(\"td\", {\"rowspan\": \"4\"})[3].text\n",
    "\n",
    "        # チルト角度\n",
    "        race_result_dict[\"tilt\"] = row.find_all(\"td\", {\"rowspan\": \"4\"})[4].text\n",
    "        \n",
    "        # 展示競争での進入コース\n",
    "        race_result_dict[\"exhibition_cource\"] = row2.find_all(\"span\")[0].text\n",
    "        # 展示start time (ST, flyng, late)\n",
    "        ex_st_ = row2.find_all(\"span\")[3].text\n",
    "        if len(ex_st_) == 3:\n",
    "            race_result_dict[\"exhibition_ST\"] = ex_st_\n",
    "            race_result_dict[\"flying\"] = 0\n",
    "            race_result_dict[\"late\"] = 0\n",
    "\n",
    "        elif len(ex_st_) == 4:\n",
    "            race_result_dict[\"exhibition_ST\"] = ex_st_[1:]\n",
    "            if ex_st_[0] == \"F\":\n",
    "                race_result_dict[\"flying\"] = 1\n",
    "                race_result_dict[\"late\"] = 0\n",
    "            # elif ex_st_[0] == \"L\":\n",
    "            #     race_result_dict[\"late_{0}\".format(i)] = 1\n",
    "            else:\n",
    "                raise Exception(\"{0}号艇ex_stが予定外（{1}）\".format(i, ex_st_))\n",
    "        elif len(ex_st_) == 1:\n",
    "            if ex_st_[0] == \"L\":\n",
    "                race_result_dict[\"exhibition_ST\"] = None\n",
    "                race_result_dict[\"late\"] = 1\n",
    "            else:\n",
    "                raise Exception(\"{0}号艇ex_stが予定外（{1}）\".format(i, ex_st_))\n",
    "\n",
    "        else:\n",
    "            raise Exception(\"{0}号艇ex_stが予定外（{1}）\".format(i, ex_st_))\n",
    "        \n",
    "        # 最初に定義したリストに辞書型のデータを追加\n",
    "        race_result_dict_list.append(race_result_dict)\n",
    "\n",
    "    # dictを入れたlistをdfに変換\n",
    "    beforeinfo_df = pd.DataFrame.from_dict(race_result_dict_list)\n",
    "\n",
    "    time.sleep(0.1)\n",
    "\n",
    "    return beforeinfo_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_raceresult(soup, rno, jcd, hd):\n",
    "    \n",
    "    race_result_dict_list = []\n",
    "    \n",
    "    table = soup.find(class_=\"contentsFrame1_inner\").find_all(class_=\"table1\")[1]\n",
    "    rows = table.find_all(\"tbody\")\n",
    "    \n",
    "    for row in rows:\n",
    "        race_result_dict = {\"date\": \"-\".join([hd[0:4], hd[5:7], hd[8:10]]),\n",
    "                            \"venue\": jcd,\n",
    "                            \"raceNumber\": rno[:-1]\n",
    "                            }\n",
    "        race_result_dict[\"着順\"] = row.find_all(\"td\")[0].text\n",
    "        # 枠番はintegerにしておかないとconcatした時に別の行として扱われてしまう\n",
    "        race_result_dict[\"枠\"] = int(row.find_all(\"td\")[1].text)\n",
    "        race_result_dict[\"タイム\"] = row.find_all(\"td\")[3].text\n",
    "        \n",
    "        # 最初に定義したリストに辞書型のデータを追加\n",
    "        race_result_dict_list.append(race_result_dict)\n",
    "    \n",
    "    # dictを入れたlistをdfに変換\n",
    "    raceresult_df = pd.DataFrame.from_dict(race_result_dict_list)\n",
    "\n",
    "    time.sleep(0.1)\n",
    "\n",
    "    return raceresult_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### そのほかcrawl, scrapeに必要なモジュール"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_url(crawl_key, rno, jcd, hd):\n",
    "    \"\"\"\n",
    "    :param crawl_key: 何をcrawleするか。選択肢は、\"odds3t\"（オッズ）, \"racelist\"(出走表）,\n",
    "    \"beforeinfo\" (直前情報）もしくは\"raceresult\" (レース結果)\n",
    "    :param rno: レース番号。8Rなど、1-12の数字 + R をstrで\n",
    "    :param jcd: 会場名。\"桐　生\"、\"びわこ\"など\n",
    "    :param hd: holding day (レース開催日)、2019/03/28などyyyy/mm/ddの形で入力（strで）\n",
    "    :return dds_url: 公式サイト最終オッズが書かれているページのurl. これを使ってcrawlする\n",
    "    \"\"\"\n",
    "    jcd_dict =  {\"桐　生\": \"01\", \"戸　田\": \"02\", \"江戸川\": \"03\", \"平和島\": \"04\", \"多摩川\": \"05\", \"浜名湖\": \"06\", \"蒲　郡\": \"07\", \"常　滑\": \"08\",\n",
    "                \"　津　\": \"09\", \"三　国\": \"10\", \"びわこ\": \"11\", \"住之江\": \"12\", \"尼　崎\": \"13\", \"鳴　門\": \"14\", \"丸　亀\": \"15\", \"児　島\": \"16\",\n",
    "                \"宮　島\": \"17\", \"徳　山\": \"18\", \"下　関\": \"19\", \"若　松\": \"20\", \"芦　屋\": \"21\", \"福　岡\": \"22\", \"唐　津\": \"23\", \"大　村\": \"24\"\n",
    "                }\n",
    "    rno = rno[:-1]\n",
    "    hd = hd[0:4] + hd[5:7] + hd[8:10]\n",
    "\n",
    "    odds_url = \"http://boatrace.jp/owpc/pc/race/\" + crawl_key + \"?rno=\" + rno + \"&jcd=\" + jcd_dict[jcd] + \"&hd=\" + hd\n",
    "\n",
    "    return odds_url\n",
    "\n",
    "\n",
    "def html_parser(site_url):\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:47.0) Gecko/20100101 Firefox/47.0\",\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        request = urllib.request.Request(url=site_url, headers=headers)\n",
    "        response = urllib.request.urlopen(request)\n",
    "\n",
    "        html = response.read().decode('utf-8')\n",
    "        soup = BeautifulSoup(html, 'lxml')\n",
    "\n",
    "    # データベース作成の際、remotedisconnectedになった場合,そのレースをパス\n",
    "    except RemoteDisconnected:\n",
    "        print(\"remote disconnected error !\")\n",
    "        return None\n",
    "\n",
    "    except ConnectionResetError:\n",
    "        print(\"Connection Reset error !\")\n",
    "        return None\n",
    "\n",
    "    return soup\n",
    "\n",
    "def get_extractor(crawl_key):\n",
    "    \n",
    "    \"\"\"\n",
    "    クロール先に応じたcrawlerを用意\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    extractor_dict = {\"racelist\": scrape_racelist,\n",
    "                      \"beforeinfo\": scrape_beforeinfo,\n",
    "                      \"raceresult\": scrape_raceresult,\n",
    "                      }\n",
    "    \n",
    "    return extractor_dict[crawl_key]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 実行部分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021/02/23 のデータをクロール中\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61f3e23f4d41489bab1371bbf188b92b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/24 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021/02/24 のデータをクロール中\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8330cab4f184e159353642be9dc438e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/24 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "hd_list = [\"2021/02/\" + str(day) for day in range(23,29)]\n",
    "\n",
    "crawl_key_list = [\"racelist\", \"beforeinfo\", \"raceresult\"]\n",
    "jcd_list =  [\"桐　生\", \"戸　田\", \"江戸川\", \"平和島\", \"多摩川\", \"浜名湖\", \"蒲　郡\", \"常　滑\",\n",
    "                \"　津　\", \"三　国\", \"びわこ\", \"住之江\", \"尼　崎\", \"鳴　門\", \"丸　亀\", \"児　島\",\n",
    "                \"宮　島\", \"徳　山\", \"下　関\", \"若　松\", \"芦　屋\", \"福　岡\", \"唐　津\", \"大　村\"\n",
    "            ]\n",
    "\n",
    "for hd in hd_list:\n",
    "    print(\"{0} のデータをクロール中\".format(hd))\n",
    "\n",
    "    # 1日単位でデータを集めてファイルに保存する\n",
    "    today_race_df_list = []\n",
    "\n",
    "    for jcd in tqdm(jcd_list):\n",
    "        for i in range(1, 13):\n",
    "            rno = str(i) + \"R\"\n",
    "\n",
    "            # その日レースがない場所は飛ばすためのtry-except         \n",
    "            try:\n",
    "                # 色々なkeyに対してクロールして特定のレースの情報がまとまったdfを作る\n",
    "                race_info_df_list = []\n",
    "\n",
    "                for crawl_key in crawl_key_list:\n",
    "                    raceResult_url = make_url(crawl_key, rno, jcd, hd)\n",
    "\n",
    "                    # パース\n",
    "                    soup = html_parser(raceResult_url)\n",
    "\n",
    "                    # extractorの指定\n",
    "                    the_extractor = get_extractor(crawl_key)\n",
    "\n",
    "                    # 対象サイトをcrawl\n",
    "                    race_information_df = the_extractor(soup, rno, jcd, hd)\n",
    "                    race_information_df = race_information_df.set_index([\"date\", \"venue\", \"raceNumber\", \"枠\"])\n",
    "\n",
    "                    race_info_df_list.append(race_information_df)\n",
    "\n",
    "                this_race_df =pd.concat(race_info_df_list, axis=1)\n",
    "                # 今回のレースのデータを本日のデータを集めたリストに格納\n",
    "                today_race_df_list.append(this_race_df)\n",
    "\n",
    "            except IndexError:\n",
    "                # print(hd + \" \" + jcd + rno +\"データなし\")\n",
    "                pass\n",
    "\n",
    "    # 本日のレースデータを集めたリストをdfに変換    \n",
    "    today_race_df = pd.concat(today_race_df_list, axis = 0)\n",
    "\n",
    "    # pickleファイルで保存\n",
    "    today_race_df.to_pickle('./crawledData/{0}.pkl'.format(\"\".join(hd.split(\"/\"))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ファイル内容確認用\n",
    "pd.read_pickle('./crawledData/{0}.pkl'.format(\"\".join(hd.split(\"/\"))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# crawlerの動きを確認する用\n",
    "\n",
    "crawl_key = \"raceresult\"\n",
    "jcd =  \"戸　田\"\n",
    "hd = \"2021/03/10\"\n",
    "rno = \"1R\"\n",
    "\n",
    "raceResult_url = make_url(crawl_key, rno, jcd, hd)\n",
    "print(raceResult_url)\n",
    "\n",
    "# パース\n",
    "soup = html_parser(raceResult_url)\n",
    "\n",
    "# extractorの指定\n",
    "the_extractor = get_extractor(crawl_key)\n",
    "\n",
    "# 対象サイトをcrawl\n",
    "race_information_df = the_extractor(soup, rno, jcd, hd)\n",
    "race_information_df = race_information_df.set_index([\"date\", \"venue\", \"raceNumber\", \"枠\"])\n",
    "race_information_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
